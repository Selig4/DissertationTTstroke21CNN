{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "778a7f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version :  3.8.8\n",
      "OpenCV version  :  4.5.4\n",
      "Torch version :  1.10.1+cu113\n",
      "Nb of threads for OpenCV :  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "import torch.cuda \n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from utils import *\n",
    "#from model import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import random\n",
    "import sys\n",
    "from shutil import rmtree\n",
    "import matplotlib\n",
    "import logging\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.axisartist as AA\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Python version : ', platform.python_version())\n",
    "print('OpenCV version  : ', cv2.__version__)\n",
    "print('Torch version : ', torch.__version__)\n",
    "# Opencv use several cpus by default for simple operation. Using only one allows loading data in parallel much faster\n",
    "cv2.setNumThreads(0)\n",
    "print('Nb of threads for OpenCV : ', cv2.getNumThreads())\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c464ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_training(seed):\n",
    "    gc.collect()\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def print_and_log(message, log=None):\n",
    "    print(message)\n",
    "    if log is not None:\n",
    "        log.info(message)\n",
    "\n",
    "def setup_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    fileHandler = logging.FileHandler(log_file, mode='w')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    return l\n",
    "\n",
    "def close_log(log):\n",
    "    if log is not None:\n",
    "        x = list(log.handlers)\n",
    "        for i in x:\n",
    "            log.removeHandler(i)\n",
    "            i.flush()\n",
    "            i.close()\n",
    "\n",
    "def make_new_path(path):\n",
    "    if os.path.exists(path):\n",
    "        rmtree(path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "\n",
    "def make_path(path) :\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "class ActivePool(object):\n",
    "    def __init__(self):\n",
    "        super(ActivePool, self).__init__()\n",
    "        self.active=[]\n",
    "        self.running_time=[]\n",
    "        self.lock=threading.Lock()\n",
    "    def makeActive(self, name):\n",
    "        with self.lock:\n",
    "            self.active.append(name)\n",
    "    def makeInactive(self, name):\n",
    "        with self.lock:\n",
    "            self.active.remove(name)\n",
    "    def numActive(self):\n",
    "        with self.lock:\n",
    "            return len(self.active)\n",
    "    def __str__(self):\n",
    "        with self.lock:\n",
    "            return str(self.active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6027085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(count, total, title, completed=0, log=None):\n",
    "    terminal_size = get_terminal_size()\n",
    "    percentage = int(100.0 * count / total)\n",
    "    length_bar = min([max([3, terminal_size[0] - len(title) - len(str(total)) - len(str(count)) - len(str(percentage)) - 10]),20])\n",
    "    filled_len = int(length_bar * count / total)\n",
    "    bar = 'â–ˆ' * filled_len + ' ' * (length_bar - filled_len)\n",
    "    sys.stdout.write('%s [%s] %s %% (%d/%d)\\r' % (title, bar, percentage, count, total))\n",
    "    sys.stdout.flush()\n",
    "    if completed:\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        if log is not None:\n",
    "            log.info('%s [%s] %s %% (%d/%d)' % (title, bar, percentage, count, total))\n",
    "\n",
    "\n",
    "def _get_terminal_size_windows():\n",
    "    try:\n",
    "        from ctypes import windll, create_string_buffer\n",
    "        h = windll.kernel32.GetStdHandle(-12)\n",
    "        csbi = create_string_buffer(22)\n",
    "        res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)\n",
    "        if res:\n",
    "            (bufx, bufy, curx, cury, wattr,\n",
    "             left, top, right, bottom,\n",
    "             maxx, maxy) = struct.unpack(\"hhhhHhhhhhh\", csbi.raw)\n",
    "            sizex = right - left + 1\n",
    "            sizey = bottom - top + 1\n",
    "            return sizex, sizey\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_terminal_size():\n",
    "    current_os = platform.system()\n",
    "    tuple_xy = None\n",
    "    if current_os == 'Windows':\n",
    "        tuple_xy = _get_terminal_size_windows()\n",
    "        if tuple_xy is None:\n",
    "            tuple_xy = _get_terminal_size_tput()\n",
    "            # needed for window's python in cygwin's xterm!\n",
    "    if current_os in ['Linux', 'Darwin'] or current_os.startswith('CYGWIN'):\n",
    "        tuple_xy = _get_terminal_size_linux()\n",
    "    if tuple_xy is None:\n",
    "        tuple_xy = (80, 25)      # default value\n",
    "    return tuple_xy\n",
    "\n",
    "def _get_terminal_size_tput():\n",
    "    try:\n",
    "        cols = int(subprocess.check_call(shlex.split('tput cols')))\n",
    "        rows = int(subprocess.check_call(shlex.split('tput lines')))\n",
    "        return (cols, rows)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def _get_terminal_size_linux():\n",
    "    def ioctl_GWINSZ(fd):\n",
    "        try:\n",
    "            import fcntl, termios, struct\n",
    "            cr = struct.unpack('hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))\n",
    "            return cr\n",
    "        except:\n",
    "            pass\n",
    "    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)\n",
    "    if not cr:\n",
    "        try:\n",
    "            fd = os.open(os.ctermid(), os.O_RDONLY)\n",
    "            cr = ioctl_GWINSZ(fd)\n",
    "            os.close(fd)\n",
    "        except:\n",
    "            pass\n",
    "    if not cr:\n",
    "        try:\n",
    "            cr = (os.environ['LINES'], os.environ['COLUMNS'])\n",
    "        except:\n",
    "            return None\n",
    "    return int(cr[1]), int(cr[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dad851eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_figure(loss_train, loss_val, acc_val, acc_train, path_to_save):\n",
    "\n",
    "    host = host_subplot(111, axes_class=AA.Axes)\n",
    "    par = host.twinx()\n",
    "\n",
    "    host.set_xlabel(\"Epochs\")\n",
    "    host.set_ylabel(\"Loss\")\n",
    "    par.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    par.axis[\"right\"].toggle(all=True)\n",
    "\n",
    "    epochs = [i for i in range(1, len(loss_val)+1)]\n",
    "\n",
    "    host.set_xlim(1, len(epochs))\n",
    "    host.set_ylim(0, np.max([np.max(loss_train), np.max(loss_val)]))\n",
    "    par.set_ylim(0, 1)\n",
    "\n",
    "    max_acc = max(acc_val)\n",
    "    max_acc_idx = epochs[acc_val.index(max_acc)]\n",
    "    host.set_title(\"Max Validation Accuracy: %.1f%% at iteration %d\" % (max_acc*100, max_acc_idx))\n",
    "\n",
    "    host.plot(epochs, loss_train, label=\"Train loss\", linewidth=1.5)\n",
    "    host.plot(epochs, loss_val, label=\"Validation loss\", linewidth=1.5)\n",
    "    par.plot(epochs, acc_val, label=\"Validation Accuracy\", linewidth=1.5)\n",
    "    par.plot(epochs, acc_train, label=\"Train Accuracy\", linewidth=1.5)\n",
    "\n",
    "    host.legend(loc='lower right', ncol=1, fancybox=False, shadow=True)\n",
    "\n",
    "    plt.savefig(path_to_save)\n",
    "    plt.close('all')\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef90be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, path, normalize=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting 'normalize=True'.\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = np.mean(np.array([cm[i,i] for i in range(len(cm))]).sum()/cm.sum()) * 100\n",
    "\n",
    "    if normalize:\n",
    "        cm_txt = cm.astype('float') / np.array([max(tmp,1) for tmp in cm.sum(axis=1)[:, np.newaxis]]).astype('float')\n",
    "    else:\n",
    "        cm_txt = cm\n",
    "    \n",
    "    cm = cm.astype('float') / np.array([max(tmp,1) for tmp in cm.sum(axis=1)[:, np.newaxis]]).astype('float')\n",
    "    acc_2 = np.array([cm[i,i] for i in range(len(cm))])\n",
    "\n",
    "    title = 'Accuracy of %.1f%% ($\\\\mu$ = %.1f with $\\\\sigma$ = %.1f)' % (acc, np.mean(acc_2)*100, np.std(acc_2)*100)\n",
    "    plt.subplots(figsize=(12,12))\n",
    "\n",
    "    plt.imshow(cm.astype('float'), interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=14)\n",
    "    plt.yticks(tick_marks, classes, fontsize=14)\n",
    "\n",
    "    fmt = '.2g' if normalize else 'd'\n",
    "    thresh = .5\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, format(round(cm_txt[i, j]*100,2), fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, format(round(cm_txt[i, j],2), fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "170c8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extractor(video_path, width, save_path):\n",
    "    # Load Video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length_video = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number = 0\n",
    "\n",
    "    # Check if video uploaded\n",
    "    if not cap.isOpened():\n",
    "        sys.exit(\"Unable to open the video, check the path.\\n\")\n",
    "\n",
    "    while frame_number < length_video:\n",
    "\n",
    "        # Load video\n",
    "        _, rgb = cap.read()\n",
    "\n",
    "        # Check if load Properly\n",
    "        if _ == 1:\n",
    "            # Resizing and Save\n",
    "            rgb = cv2.resize(rgb, (width, rgb.shape[0]*width//rgb.shape[1]))\n",
    "            cv2.imwrite(os.path.join(save_path, '%08d.png' % frame_number), rgb)\n",
    "            frame_number+=1\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5292b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_features(x):\n",
    "    size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "    num_features = 1\n",
    "    for s in size:\n",
    "        num_features *= s\n",
    "    return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fc65a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSimpleBranch(nn.Module):\n",
    "    def __init__(self, size_data, n_classes, channels=3):\n",
    "        super(NetSimpleBranch, self).__init__()\n",
    "\n",
    "        ####################\n",
    "        ####### First ######\n",
    "        ####################\n",
    "        self.conv1 = nn.Conv3d(channels, 30, (3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d((2, 2, 2), stride=(2, 2, 2))\n",
    "        size_data //= 2\n",
    "\n",
    "        ####################\n",
    "        ###### Second ######\n",
    "        ####################\n",
    "        self.conv2 = nn.Conv3d(30, 60, (3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)) \n",
    "        self.pool2 = nn.MaxPool3d((2, 2, 2), stride=(2, 2, 2))\n",
    "        size_data //= 2\n",
    "\n",
    "        ####################\n",
    "        ####### Third ######\n",
    "        ####################\n",
    "        self.conv3 = nn.Conv3d(60, 80, (3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)) \n",
    "        self.pool3 = nn.MaxPool3d((2, 2, 2), stride=(2, 2, 2))\n",
    "        size_data //= 2\n",
    "\n",
    "        ####################\n",
    "        ####### Last #######\n",
    "        ####################\n",
    "        self.linear1 = nn.Linear(80*size_data[0]*size_data[1]*size_data[2], 500)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fusion\n",
    "        self.linear2 = nn.Linear(500, n_classes)\n",
    "        self.final = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        ####################\n",
    "        ####### First ######\n",
    "        ####################\n",
    "        data = self.pool1(F.relu(self.conv1(data)))\n",
    "\n",
    "        ####################\n",
    "        ###### Second ######\n",
    "        ####################\n",
    "        data = self.pool2(F.relu(self.conv2(data)))\n",
    "\n",
    "        ####################\n",
    "        ####### Third ######\n",
    "        ####################\n",
    "        data = self.pool3(F.relu(self.conv3(data)))\n",
    "\n",
    "\n",
    "        ####################\n",
    "        ####### Last #######\n",
    "        ####################\n",
    "        data = data.view(-1, flatten_features(data))\n",
    "        data = self.relu(self.linear1(data))\n",
    "\n",
    "        data = self.linear2(data)\n",
    "        label = self.final(data)\n",
    "\n",
    "        return label\n",
    "        print(model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ef03add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_variables():\n",
    "    def __init__(self, task_path, size_data=[98, 120, 120], cuda=True, batch_size=15, workers=6, epochs=1000, lr=0.0001, nesterov=True, weight_decay=0.005, momentum=0.5):\n",
    "        self.cuda = cuda\n",
    "        self.workers = workers\n",
    "        self.batch_size = batch_size\n",
    "        self.size_data = np.array(size_data)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.nesterov = nesterov\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.model_name = os.path.join(task_path, 'MediaEval21_%s' % (datetime.datetime.now().strftime('%d-%m-%Y_%H-%M')))\n",
    "        make_path(task_path)\n",
    "        make_path(self.model_name)\n",
    "        if cuda:\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "            # os.environ[ 'CUDA_VISIBLE_DEVICES' ] = '0'\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "        self.log = setup_logger('model_log', os.path.join(self.model_name, 'model_log.log'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "104a9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' My_dataset class which uses My_stroke class to be used in the data loader'''\n",
    "class My_dataset(Dataset):\n",
    "    def __init__(self, dataset_list, size_data):\n",
    "        self.dataset_list = dataset_list\n",
    "        self.size_data = size_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb = get_data(self.dataset_list[idx].video_path, self.dataset_list[idx].begin, self.size_data)\n",
    "        sample = {'rgb': torch.FloatTensor(rgb), 'label' : self.dataset_list[idx].move, 'my_stroke' : {'video_path':self.dataset_list[idx].video_path, 'begin':self.dataset_list[idx].begin, 'end':self.dataset_list[idx].end}}\n",
    "        return sample\n",
    "\n",
    "''' My_stroke class used for encoding the annotations'''\n",
    "class My_stroke:\n",
    "    def __init__(self, video_path, begin, end, move):\n",
    "        self.video_path = video_path\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "        self.move = move\n",
    "\n",
    "    def my_print(self, log=None):\n",
    "        print_and_log('Video : %s\\tbegin : %d\\tEnd : %d\\tClass : %s' % (self.video_path, self.begin, self.end, self.move), log=log)\n",
    "\n",
    "''' Get annotations from xml files located in one folder and produce a list of My_stroke'''\n",
    "def get_annotations(xml_path, data_folder, list_of_strokes=None):\n",
    "    xml_list = [os.path.join(xml_path, f) for f in os.listdir(xml_path) if os.path.isfile(os.path.join(xml_path, f)) and f.split('.')[-1]=='xml']\n",
    "    strokes_list = []\n",
    "    for xml_file in xml_list:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        video_path = os.path.join(data_folder, xml_file.split('/')[-1].split('.')[0])\n",
    "        for action in root:\n",
    "            if list_of_strokes is None:\n",
    "                strokes_list.append(My_stroke(video_path, int(action.get('begin')), int(action.get('end')), 1))\n",
    "            else:\n",
    "                strokes_list.append(My_stroke(video_path, int(action.get('begin')), int(action.get('end')), list_of_strokes.index(action.get('move'))))\n",
    "        # Case of the test set in segmentation task - build proposals of size 150\n",
    "        if len(root)==0: \n",
    "            for begin in range(0,len(os.listdir(video_path))-150,150):\n",
    "                strokes_list.append(My_stroke(video_path, begin, begin+150, 0))\n",
    "    return strokes_list\n",
    "\n",
    "'''Infer Negative Samples from annotation betwen strokes when there are more than length_min frames'''\n",
    "def build_negative_strokes(stroke_list, length_min=200):\n",
    "    video_path = 'tmp'\n",
    "    for stroke in stroke_list.copy():\n",
    "        if stroke.video_path != video_path:\n",
    "            video_path = stroke.video_path\n",
    "            begin_negative = 0\n",
    "        end_negative = stroke.begin\n",
    "        for end in range(begin_negative+length_min, end_negative, length_min):\n",
    "            stroke_list.append(My_stroke(video_path, end-length_min, end, 0))\n",
    "        begin_negative = stroke.end\n",
    "\n",
    "''' Get the rgb frames from the annotations'''\n",
    "def get_data(data_path, begin, size_data):\n",
    "    rgb_data = []\n",
    "    for frame_number in range(begin, begin + size_data[0]):\n",
    "        try:\n",
    "            rgb = cv2.imread(os.path.join(data_path, '%08d.png' % frame_number))\n",
    "            rgb = cv2.resize(rgb, (size_data[1], size_data[2])).astype(float) / 255\n",
    "        except:\n",
    "            raise ValueError('Problem with %s begin %d size %d' % (os.path.join(data_path, '%08d.png' % frame_number), begin, size_data[0]))\n",
    "        rgb_data.append(cv2.split(rgb))\n",
    "\n",
    "    rgb_data = np.transpose(rgb_data, (1, 0, 2, 3))\n",
    "    return rgb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06dbd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_architecture(args, output_size):\n",
    "    print_and_log('Make Model', log=args.log)\n",
    "    model = NetSimpleBranch(args.size_data.copy(), output_size)\n",
    "    ## Use GPU\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6860f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training is split in train epoch and validation epoch and produce a plot'''\n",
    "def train_model(model, args, train_loader, valid_loader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=args.nesterov)\n",
    "\n",
    "    begin_time = time.time()\n",
    "    print_and_log('\\nTraining...', log=args.log)\n",
    "\n",
    "    # For plot\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train and validation step and save loss and acc for plot\n",
    "        loss_train_, acc_train_ = train_epoch(epoch, args, model, train_loader, optimizer, criterion)\n",
    "        loss_val_, acc_val_ = validation_epoch(epoch, args, model, valid_loader, criterion)\n",
    "\n",
    "        loss_train.append(loss_train_)\n",
    "        acc_train.append(acc_train_)\n",
    "        loss_val.append(loss_val_)\n",
    "        acc_val.append(acc_val_)\n",
    "    print_and_log('Max validation accuracy of %.2f done in %ds' % (max(acc_val), int(time.time()-begin_time)), log=args.log)\n",
    "    make_train_figure(loss_train, loss_val, acc_val, acc_train, os.path.join(args.model_name, 'Train.png'))\n",
    "    return 1\n",
    "\n",
    "''' Update of the model in one epoch'''\n",
    "def train_epoch(epoch, args, model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    pid = os.getpid()\n",
    "    N = len(data_loader.dataset)\n",
    "    begin_time = time.time()\n",
    "    aLoss = 0\n",
    "    Acc = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        # Get batch tensor\n",
    "        rgb, label = batch['rgb'], batch['label']\n",
    "\n",
    "        rgb = Variable(rgb.type(args.dtype))\n",
    "        label = Variable(label.type(args.dtype).long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(rgb)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        aLoss += loss.item()\n",
    "        Acc += output.data.max(1)[1].eq(label.data).cpu().sum().numpy()\n",
    "        progress_bar((batch_idx + 1) * args.batch_size, N, '%d Training - Epoch : %d - Batch Loss = %.5g' % (pid, epoch, loss.item()))\n",
    "\n",
    "    aLoss /= N\n",
    "    progress_bar(N, N, 'Train - Epoch %d - Loss = %.5g - Accuracy = %.3g (%d/%d) - Time = %ds' % (epoch, aLoss, Acc/N, Acc, N, time.time() - begin_time), 1, log=args.log)\n",
    "    return aLoss, Acc/N\n",
    "\n",
    "\n",
    "'''Validation of the model in one epoch'''\n",
    "def validation_epoch(epoch, args, model, data_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        begin_time = time.time()\n",
    "        pid = os.getpid()\n",
    "        N = len(data_loader.dataset)\n",
    "        _loss = 0\n",
    "        _acc = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            progress_bar(batch_idx*args.batch_size, N, '%d - Validation' % (pid))\n",
    "            rgb, label = batch['rgb'], batch['label']\n",
    "            rgb = Variable(rgb.type(args.dtype))\n",
    "            label = Variable(label.type(args.dtype).long())\n",
    "            output = model(rgb)\n",
    "            _loss += criterion(output, label).item()\n",
    "            output_indexes = output.data.max(1)[1]\n",
    "            _acc += output.data.max(1)[1].eq(label.data).cpu().sum().numpy()\n",
    "\n",
    "        _loss /= N\n",
    "        progress_bar(N, N, 'Validation - Loss = %.5g - Accuracy = %.3g (%d/%d) - Time = %ds' % (_loss, _acc/N, _acc, N, time.time() - begin_time), 1, log=args.log)\n",
    "        return _loss, _acc/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8af67085",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Store data for xml files from the list of stroke with predicted class - for detection it is saved when index predicted to 1'''\n",
    "def store_xml_data(my_stroke_list, predicted, xml_files, list_of_strokes=None):\n",
    "    for video_path, begin, end, prediction_index in zip(my_stroke_list['video_path'], my_stroke_list['begin'].tolist(), my_stroke_list['end'].tolist(), predicted):\n",
    "        video_name = video_path.split('/')[-1]\n",
    "        if video_name not in xml_files:\n",
    "            xml_files[video_name] = ET.Element('video')\n",
    "        if list_of_strokes is None:\n",
    "            if prediction_index:\n",
    "                stroke_xml = ET.SubElement(xml_files[video_name], 'action')\n",
    "                stroke_xml.set('begin', str(begin))\n",
    "                stroke_xml.set('end', str(end))\n",
    "        else:\n",
    "            stroke_xml = ET.SubElement(xml_files[video_name], 'action')\n",
    "            stroke_xml.set('begin', str(begin))\n",
    "            stroke_xml.set('end', str(end))\n",
    "            stroke_xml.set('move', list_of_strokes[prediction_index])\n",
    "\n",
    "'''Save the predictions in xml files'''\n",
    "def save_xml_data(xml_files, path_xml_save):\n",
    "    for video_name in xml_files:\n",
    "        xml_file = open('%s.xml' % os.path.join(path_xml_save, video_name), 'wb')\n",
    "        xml_file.write(ET.tostring(xml_files[video_name]))\n",
    "        xml_file.close()\n",
    "\n",
    "'''Inference on test set'''\n",
    "def test_model(model, args, data_loader, list_of_strokes=None):\n",
    "    with torch.no_grad():\n",
    "        xml_files = {}\n",
    "        path_xml_save = os.path.join(args.model_name, 'xml_test')\n",
    "        make_path(path_xml_save)\n",
    "        N = len(data_loader.dataset)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # Get batch tensor\n",
    "            rgb, my_stroke_list = batch['rgb'], batch['my_stroke']\n",
    "            progress_bar(args.batch_size*batch_idx, N, 'Testing')\n",
    "\n",
    "            rgb = Variable(rgb.type(args.dtype))\n",
    "            output = model(rgb)\n",
    "            _, predicted = torch.max(output.detach(), 1)\n",
    "            store_xml_data(my_stroke_list, predicted, xml_files, list_of_strokes)\n",
    "\n",
    "        progress_bar(N, N, 'Test done', 1, log=args.log)\n",
    "        save_xml_data(xml_files, path_xml_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a46dbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set up the environment and extract data'''\n",
    "def make_work_tree(main_folder, source_folder, frame_width=320, extract=False):\n",
    "    data_path = os.path.join(main_folder, 'data')\n",
    "    video_folder = os.path.join(source_folder, 'videos')\n",
    "    detection_path = os.path.join(source_folder,'detectionTask')\n",
    "    classification_path = os.path.join(source_folder,'classificationTask')\n",
    "    if extract:\n",
    "        make_path(main_folder)\n",
    "        make_path(data_path)\n",
    "        video_list = [_file for _file in os.listdir(video_folder) if _file[-4:]=='.mp4' and os.path.isfile(os.path.join(video_folder, _file))]\n",
    "        for idx, video in enumerate(video_list):\n",
    "            save_frame_path = os.path.join(data_path, video[:-4])\n",
    "            make_path(save_frame_path)\n",
    "            progress_bar(idx, len(video_list), 'Frame extraction')\n",
    "            frame_extractor(os.path.join(video_folder, video), frame_width, save_frame_path)\n",
    "        progress_bar(len(video_list), len(video_list), 'Frame extraction done', 1)\n",
    "    return main_folder, data_path, detection_path, classification_path\n",
    "\n",
    "''' According to overview paper'''\n",
    "def get_list_of_strokes():\n",
    "    list_of_strokes = ['Serve Forehand Backspin',\n",
    "                   'Serve Forehand Loop',\n",
    "                   'Serve Forehand Sidespin',\n",
    "                   'Serve Forehand Topspin',\n",
    "\n",
    "                   'Serve Backhand Backspin',\n",
    "                   'Serve Backhand Loop',\n",
    "                   'Serve Backhand Sidespin',\n",
    "                   'Serve Backhand Topspin',\n",
    "\n",
    "                   'Offensive Forehand Hit',\n",
    "                   'Offensive Forehand Loop',\n",
    "                   'Offensive Forehand Flip',\n",
    "\n",
    "                   'Offensive Backhand Hit',\n",
    "                   'Offensive Backhand Loop',\n",
    "                   'Offensive Backhand Flip',\n",
    "\n",
    "                   'Defensive Forehand Push',\n",
    "                   'Defensive Forehand Block',\n",
    "                   'Defensive Forehand Backspin',\n",
    "\n",
    "                   'Defensive Backhand Push',\n",
    "                   'Defensive Backhand Block',\n",
    "                   'Defensive Backhand Backspin',\n",
    "                   'Unknown']\n",
    "    return list_of_strokes\n",
    "\n",
    "'''Get the split of annotation and construct negative samples fro; it if in dectetion task'''\n",
    "def get_lists_annotations(task_path, data_path, list_of_strokes=None):\n",
    "    train_strokes = get_annotations(os.path.join(task_path, 'train'), data_path, list_of_strokes)\n",
    "    valid_strokes = get_annotations(os.path.join(task_path, 'valid'), data_path, list_of_strokes)\n",
    "    test_strokes = get_annotations(os.path.join(task_path, 'test'), data_path, list_of_strokes)\n",
    "    if list_of_strokes is None:\n",
    "        build_negative_strokes(train_strokes)\n",
    "        build_negative_strokes(valid_strokes)\n",
    "        build_negative_strokes(test_strokes)\n",
    "    return train_strokes, valid_strokes, test_strokes\n",
    "\n",
    "''' Build dataloader from list of strokes'''\n",
    "def get_data_loaders(train_strokes, valid_strokes, test_strokes, size_data, batch_size, workers):\n",
    "    # Build Dataset\n",
    "    train_set = My_dataset(train_strokes, size_data)\n",
    "    valid_set = My_dataset(valid_strokes, size_data)\n",
    "    test_set = My_dataset(test_strokes, size_data)\n",
    "\n",
    "    # Loaders of the Datasets\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=0)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "'''Classification task'''\n",
    "def classification_task(main_folder, data_path, task_path):\n",
    "    print('\\nClassification Task')\n",
    "    # Initial list\n",
    "    reset_training(1)\n",
    "    list_of_strokes = get_list_of_strokes()\n",
    "\n",
    "    # Split\n",
    "    train_strokes, valid_strokes, test_strokes = get_lists_annotations(task_path, data_path, list_of_strokes)\n",
    "    \n",
    "    # Model variables\n",
    "    args = my_variables('classificationTask')\n",
    "    \n",
    "    ## Architecture with the output of the lenght of possible classes - (Unknown not counted)\n",
    "    model = make_architecture(args, len(list_of_strokes)-1)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader, valid_loader, test_loader = get_data_loaders(train_strokes, valid_strokes, test_strokes, args.size_data, args.batch_size, args.workers)\n",
    "\n",
    "    # Training process\n",
    "    train_model(model, args, train_loader, valid_loader)\n",
    "    \n",
    "    # Test process \n",
    "    test_model(model, args, test_loader, list_of_strokes)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df831bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Task\n",
      "Make Model\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Problem with .\\data\\data\\classificationTask\\train\\4657106722\\00054120.png begin 54120 size 98",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-0f6d1158a0e6>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(data_path, begin, size_data)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mrgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%08d.png'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mframe_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mrgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-84b7c0f9f6ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Tasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mclassification_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint_and_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'All Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-750e29362a55>\u001b[0m in \u001b[0;36mclassification_task\u001b[1;34m(main_folder, data_path, task_path)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Training process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;31m# Test process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-65f20b9d47e3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, args, train_loader, valid_loader)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Train and validation step and save loss and acc for plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_train_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mloss_val_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-65f20b9d47e3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, args, model, data_loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Get batch tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mrgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rgb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-0f6d1158a0e6>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mrgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'rgb'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'my_stroke'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'video_path'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-0f6d1158a0e6>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(data_path, begin, size_data)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mrgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Problem with %s begin %d size %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%08d.png'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mframe_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mrgb_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Problem with .\\data\\data\\classificationTask\\train\\4657106722\\00054120.png begin 54120 size 98"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # MediaEval Task source folder\n",
    "    source_folder = 'C:/Users/s222237/OneDrive - University of Suffolk/data'\n",
    "    \n",
    "    # Prepare tree and data - To call only once with extract set to True\n",
    "    main_folder, data_path, detection_path, classification_path = make_work_tree('.', source_folder, extract=False)\n",
    "\n",
    "    # Tasks\n",
    "    classification_task(main_folder, data_path, classification_path)\n",
    "\n",
    "    print_and_log('All Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c248bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
